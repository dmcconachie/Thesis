\chapter{Discussion and Future Work}

This dissertation presented a framework for autonomously manipulating deformable objects, focusing on methods that can be used without a time-consuming modelling phase, and without high-fidelity simulation. We have focused on techniques and algorithmic choices that enable the robot to learn from performing tasks, improving the robot's ability to manipulate deformable objects and perform interesting tasks. Our contributions in modelling and control do not involve learning directly, but they form part of the basis from which the robot is able to acquire experience from which to learn. These methods could also be combined with other techniques such as residual physics~\cite{zengtossingbot} to integrate analytical methods with learning based approaches.

One of the limitations in this dissertation is the representation of the deformable object as a set of points along with the assumption that we can sense these points directly. By making these assumptions we are requiring far more from our perception systems that they are capable of in general. For example if we consider a pile of clothes, being able to disambiguate between each clothing item that the robot can see, as well as estimating the configuration of every item a prohibitive requirement. Instead, we may want to consider task specific representations that enable a higher level of reasoning over actions rather than a point-wise representation. It is an open question how to come by these representations, or how to know when to use a given representation for the task at hand. In this work, we used two representations, one for local control purposes and another for global planning. By using these different representations we were able to write algorithms focused on different parts of an overall task, and then combine these algorithms together into a single framework. Other representations may allow for a different way of thinking about deformable object manipulation that may be better suited to other tasks.

In this dissertation we hand designed the decisions for when to use each representation and the associated method for manipulating the deformable object. Extending this framework to a broader set of abilities and representations naturally leads to approaches such as task and motion planning techniques which reason over discrete choices (for example which representation/algorithm to use) along with continuous variables (for example where to grasp). A great deal more work is needed in this area as we work towards integrating deformable object manipulation tools into a general robotics solution.

Some of the limitations imposed by our interleaving framework may be addressed by closing the loop on the execution of plans generated by RRT-EB. By monitoring the deformable object as a plan is executed we can quickly determine when the deformable object has diverged from the planned path, but it is not clear how to recover from a divergence. Can we define a local controller that reasons about how close our VEB model reduction is representing the true configuration of the deformable object and close the loop at this level? While this may be possible in some circumstances, in general we believe that something like our interleaving framework will always be needed in order to respond to unmodelled aspects of problems and continue with a given task.

One of the key differences between rigid body verses deformable object manipulation is that deformable objects are compliant; we explicitly took advantage of this when designing our VEB model reduction and planner. That said, we have not taken advantage of that compliance as an explicit manipulation tool; the directional rigidity model in Section~\ref{sec:constrained_model} can enable a controller to take advantage of interactions with the environment, but it does so without explicitly reasoning about it. It's not clear how to take advantage of such contact in a principled fashion; is including contact in the modelling process enough? Is it better to use contact as a way of creating funnels for chaining actions or controllers? Can we exploit contact in a way that helps reduce the scope of the problem that a task and motion planning framework needs to solve? These are just some of the questions that arise from the inherent differences between rigid body manipulation and deformable object manipulation.