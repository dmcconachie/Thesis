\section{Related Work}

Control for deformable objects has been investigated in a variety of industrial~\cite{Henrich2000} and surgical contexts~\cite{khalil2010dexterous}. Force-based control is often used~\cite{Remde1999,Kraus1997,Tanner2006}, notably for surgical applications~\cite{Kazanzides1992,Liu2011}. There is also significant work on visual servoing for deformable objects~\cite{Hirai2000,Wada2001,Smolen2009,Navarro-Alarcon2013}. In order for any control technique to be applied, we first need some model of the deformable object.

\subsection{Deformable Object Modeling}

One of the key challenges in manipulating deformable objects is the difficulty inherent in modeling and simulating them. While there has been some progress towards online modeling of deformable objects~\cite{JochenLang2002,Cretu2008a} these methods rely on a time consuming training phase for each object to be modeled. This training phase typically consists of probing the deformable object with test forces in various configurations, and then fitting model parameters to the generated data. While this process can generate useful models, the time it takes to generate a model for each task can be prohibitive for some applications. Of particular interest are Jacobian-based models such as~\cite{Berenson2013} and~\cite{Navarro-Alarcon2013}. In these models we assume that there is some function $F : \robotconfigspace \rightarrow \reals^N$ which maps a configuration of $\numgrippers$ robot grippers $\robotconfig \in \robotconfigspace$ to a parameterization of the deformable object $\deformconfig \in \reals^N$, where $N$ is the dimensionality of the parameterization of the deformable object.  These models are then linearized by calculating an approximation of the the Jacobian of $F$:
\begin{align*}
    \deformconfig                               &= F(\robotconfig) \\
    \frac{\partial \deformconfig}{\partial t}   &= \frac{\partial F(\robotconfig)}{\partial \robotconfig} \frac{\partial \robotconfig}{\partial t} \\
    \deformvelocity                             &= J(q) \robotvelocity \enspace .\numberthis
    \label{eqn:jacobian}
\end{align*}

Computation of an exact Jacobian $J(\robotconfig)$ at a given configuration $\robotconfig$ is often computationally intractable and requires high-fidelity models and simulators, so instead approximations are frequently used. A shared characteristic of these approximations is some reliance on tuned parameters. This tuning process can be tedious, and in some cases needs to be done on a per-task basis.

In this paper we consider two types of approximate Jacobian models. The first approximation we use is a \textit{diminishing-rigidity Jacobian}~\cite{Berenson2013} which assumes that points on the deformable object that are near a gripper move ``almost rigidly'' with respect to the gripper while points that are further away move ``less rigidly''. This approximation uses deformability parameters to control how quickly the rigidity decreases with distance. The second approximation we use is an \textit{adaptive Jacobian}~\cite{Navarro-Alarcon2013} which uses online estimation to approximate $J(\robotconfig)$. Adaptive Jacobian models rely on a learning rate to control how quickly the estimation changes from one timestep to the next.

\subsection{Model Selection}

In order to accomplish a given manipulation task, we need to determine which type of model to use at the current time to compute the next velocity command, as well as how to set the model parameters. Frequently this selection is done manually, however, there are methods designed to make these determinations automatically. Machine learning techniques such as~\cite{Maron1994,Sparks2015} rely on supervised training data in order to intelligently search for the best regression or classification model. These methods are able to estimate the accuracy of each model as training data is processed, pruning models from the training that are unlikely to converge or otherwise outperform models that are kept. These methods are designed for large datasets rather than an online setting where we may not have any training data \textit{a priori}. While it may be possible to adjust these methods to consider model utility instead of model accuracy, it is unclear how to acquire the needed training data for the task at hand without having already performed the task. The most directly applicable methods come from the Multi-Armed Bandit (MAB) literature~\cite{Auer2002,Gittins2011,Whittle1988}. In this framework there are multiple actions we can take, each of which provides us with some reward according to an unknown probability distribution. The problem then is to determine which action to take (which arm to pull) at each time step in order to maximize reward.

The MAB approach is well-studied for problems where the reward distributions are \textit{stationary}; i.e. the distributions do not change over time~\cite{Auer2002,Agrawal2012}. This is not the case for deformable object manipulation; consider the situation where the object is far away from the goal versus the object being at the goal. In the first case there is a possibility of an action moving the object closer to the goal and thus achieving a positive reward; however, in the second case any motion would, at best, give zero reward. In the \textit{contextual bandits} \cite{Langford2008,Slivkins2014} variation of the MAB problem, additional contextual information or features are observed at each timestep, which can be used to determine which arm to pull. Typical solutions map the current features to estimates of the expected reward for each arm using regressions techniques or other metric-space analysis. In order to use contextual bandits for a given task a set of features would need to be engineered, however it is not clear what features to use.

Recent work~\cite{Granmo2010} on non-stationary MAB problems offer promising results that utilize independent Kalman filters as the basis for the estimation of a non-stationary reward distribution for each arm. This algorithm (KF-MANB) provides a Bayesian estimate of the reward distribution at each timestep, assuming that the reward is normally distributed. KF-MANB then performs Thompson sampling~\cite{Agrawal2012} to select which arm to pull, choosing each in proportion to the belief that it is the optimal arm. We build on this approach in this paper to produce a method that also accounts for dependencies between arms by approximating the coupling between arms at each timestep.

For the tasks we address, the reward distributions are both non-stationary as well as \textit{dependent}. Because all arms are operating on the same physical system, pulling one arm both gives us information about the distributions over other arms, as well as changing the future reward distributions of all arms. While work has been done on dependent bandits \cite{Pandey2007,Langford2008}, we are not aware of any work addressing the combination of non-stationary and dependent bandits using a regret-based formulation. Our method for model selection is inspired by KF-MANB, however we directly use coupling between models in order to form a joint reward distribution over all models. This enables a pull of a single arm to provide information about all arms, and thus we spend less time exploring the model space and more time exploiting useful models to perform the manipulation task.
