\section{Bandit-Based Model Selection}

The primary difficulty with solving~\eqref{eqn:modelselection} directly is that the effectiveness of a particular model in minimizing error is unknown. It may be the case that no model in the set produces the optimal option, however, this does not prevent a model from being useful. In particular the \textit{utility} of a model may change from one task to another, and from one configuration to another as the deformable object changes shape, and moves in and out of contact with the environment. We start by defining the utility $\utility_\modelindex(t) \in \reals$ of a model as the expected improvement in task error $\errorfunction$ if model $\modelindex$ is used to generate a robot command at time $t$. If we know which model has the highest utility then we can solve~\eqref{eqn:modelselection}. This leads to a classic exploration versus exploitation trade-off where we need to explore the space of models in order to learn which one is the most useful, while also exploiting the knowledge we have already gained.  The multi-armed bandit framework is explicitly designed to handle this trade-off.

In the MAB framework, each arm represents a model in~$\modelset$; to pull arm $\modelindex$ is to command the grippers with velocity $\robotvelocity_\modelindex(t)$ (Eq.~\ref{eqn:robotvelocity}) for 1 unit of time. We then define the \textit{reward} $\reward_\modelindex(t+1)$ after taking action $\robotvelocity_\modelindex(t)$ as the improvement in error
\begin{equation}
    \reward_\modelindex(t+1) = \errorfunction(t) - \errorfunction(t+1) = \utility_\modelindex(t) + \utilityobsnoise
    \label{eqn:observedreward}
\end{equation}
where $\utilityobsnoise$ is a zero-mean noise term. The goal is to pick a sequence of arm pulls to minimize total expected regret $\totalregret(T_f)$ over some (possibly infinite) horizon $T_f$
\begin{equation}
    E[\totalregret(T_f)] = \sum_{t=1}^{T_f} (E[\reward^*(t)] - E[\reward(t)])
    \label{eqn:totalregret}
\end{equation}
where $\reward^*(t)$ is the reward of the best model at time $t$. The next section describes how to use bandit-based model selection for deformable object manipulation.
