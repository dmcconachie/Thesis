\section{\rev{Learning Something Something}}

Robotic manipulation of deformable objects has been studied in many contexts ranging from surgery to industrial manipulation (see \cite{Khalil2010} and \cite{Sanchez2018deformablesurvey} for extensive surveys).

Much work in deformable object manipulation relies on simulating an accurate model of the object being manipulated. %Motivated by applications in computer graphics and surgical training, many methods have been developed for simulating string-like objects \cite{Bergou2008,Rungjiratananon2011} and cloth-like objects \cite{Baraff1998,Goldenthal2007}. 
The most common simulation methods use Mass-Spring models \cite{Gibson1997, Essahbi2012}, which are generally not accurate for large deformations \cite{Maris2010}, and Finite-Element (FEM) models \cite{Muller2002,Irving2004,Kaufmann2008}. FEM-based methods are widely used and physically well-founded, but they can be unstable when subject to contact constraints, which are especially important in this work. %They also require significant tuning and are very sensitive to the discretization of the object. Furthermore, such models require knowledge of the physical properties of the object, such as it's Young's modulus and friction parameters, which we do not assume are known.

Motion planning for manipulation of deformable objects is an active area of research \cite{Jimenez2012} with many sampling-based planners proposed \cite{BurchanBayazit2002,Gayle2005,Moll2006,Saha2008,Roussel2015}. 
However, all the above methods either disallow contact with the environment or rely on potentially time-consuming physical simulation of the deformable object, which is often very sensitive to physical and computational parameters that may be difficult to determine. In contrast our method builds on \cite{McConachie2020}, which uses reduced models for motion planning with far lower computational cost.



In terms of applying machine learning to control and planning, prior work has primarily used learned dynamics models for control \cite{Jia2018,Finn2017,Banijamali2017,Zhang2019, Sutanto2019}. Recent work \cite{ichter2019} has also explored planning in learned reduced space, but they do not consider the error in a reduced model's prediction when planning. Visual Planning and Acting (VPA)~\cite{vpa2019rss} learns a latent transition model based on visual input for planning. This work also uses on a classifier to prune infeasible transitions during planning. However, despite this classifier, only 15\% of generated plans were visually plausible, with only 20\% of the visually plausible plans being executable. In this paper we do not focus on learning a reduction but rather on creating a framework that can be used to overcome limitations in a given model reduction. 
%\revp{VPA uses a learned latent space, whereas we use} a hand-designed model approximation and a classifier trained against the hand designed model. %Dmitry: not clear what this last sentence is adding