\section{Using Multiple Models}

In order to accomplish a given manipulation task, we need to determine which type of model to use at the current time to compute the next velocity command, as well as how to set the model parameters. Frequently this selection is done manually, however, there are methods designed to make these determinations automatically. Machine learning techniques such as~\cite{Maron1994,Sparks2015} rely on supervised training data in order to intelligently search for the best regression or classification model. These methods are able to estimate the accuracy of each model as training data is processed, pruning models from the training that are unlikely to converge or otherwise outperform models that are kept. These methods are designed for large datasets rather than an online setting where we may not have any training data \textit{a priori}. While it may be possible to adjust these methods to consider model utility instead of model accuracy, it is unclear how to acquire the needed training data for the task at hand without having already performed the task. The most directly applicable methods come from the \ac{MAB} literature~\cite{Auer2002,Gittins2011,Whittle1988}. In this framework there are multiple actions we can take, each of which provides us with some reward according to an unknown probability distribution. The problem then is to determine which action to take (which arm to pull) at each time step in order to maximize reward.

The \ac{MAB} approach is well-studied for problems where the reward distributions are \textit{stationary}; i.e. the distributions do not change over time~\cite{Auer2002,Agrawal2012}. This is not the case for deformable object manipulation; consider the situation where the object is far away from the goal versus the object being at the goal. In the first case there is a possibility of an action moving the object closer to the goal and thus achieving a positive reward; however, in the second case any motion would, at best, give zero reward. In the \textit{contextual bandits} \cite{Langford2008,Slivkins2014} variation of the \ac{MAB} problem, additional contextual information or features are observed at each timestep, which can be used to determine which arm to pull. Typical solutions map the current features to estimates of the expected reward for each arm using regressions techniques or other metric-space analysis. In order to use contextual bandits for a given task a set of features would need to be engineered, however it is not clear what features to use.

Recent work~\cite{Granmo2010} on non-stationary \ac{MAB} problems offer promising results that utilize independent Kalman filters as the basis for the estimation of a non-stationary reward distribution for each arm. This algorithm (KF-MANB) provides a Bayesian estimate of the reward distribution at each timestep, assuming that the reward is normally distributed. KF-MANB then performs Thompson sampling~\cite{Agrawal2012} to select which arm to pull, choosing each in proportion to the belief that it is the optimal arm. We build on this approach in this paper to produce a method that also accounts for dependencies between arms by approximating the coupling between arms at each timestep.

For the tasks we address, the reward distributions are both non-stationary as well as \textit{dependent}. Because all arms are operating on the same physical system, pulling one arm both gives us information about the distributions over other arms, as well as changing the future reward distributions of all arms. While work has been done on dependent bandits \cite{Pandey2007,Langford2008}, we are not aware of any work addressing the combination of non-stationary and dependent bandits using a regret-based formulation. Our method for model selection is inspired by KF-MANB, however we directly use coupling between models in order to form a joint reward distribution over all models. This enables a pull of a single arm to provide information about all arms, and thus we spend less time exploring the model space and more time exploiting useful models to perform the manipulation task.