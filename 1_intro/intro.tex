\chapter{Introduction}

\todoin{Replace this with broad dissertation intro, not journal/conference paper intro.}

Traditional motion planning techniques such as A*, probabilistic roadmaps, and rapidly-exploring random trees were designed with rigid body motion in mind. In this framework contact with the environment is explicitly disallowed. In order to manipulate an object a typical approach is to build a model of the object being grasped; this model is then used to ensure that the object does not collide with anything during a particular motion. In contrast, when working with deformable objects, interaction with the environment is common, with the deformable object complying to the environment rather than colliding with it. This raises the question ``How accurate do our models need to be?'' There are a broad range of deformable object manipulation tasks that robots have performed without highly accurate models ranging from surgical applications~\cite{Wang2018} to knot-tying~\cite{Huang2015}. While these methods have some success they rely on either a hand designed sequence of actions (or controllers), or a time consuming data collection phase.

To address these limitations, this dissertation is motivated by two key research questions: (1) How can robots perform a broad range of tasks with deformable objects without a time consuming modelling or data collection phase? and (2) How can robots take advantage of information learned while manipulating deformable objects? By answering these questions we can take a step towards a household robot that is capable of performing a braod range of tasks without any additional training, and can improve its ability to perform the specific tasks that it commonly encounters.

Examples of deformable object manipulation range from domestic tasks like folding clothes to time and safety critical tasks such as robotic surgery. One of the challenges in planning for deformable object manipulation is the high number of degrees of freedom involved; even approximating the configuration of a piece of cloth in 3D with a 4 $\times$ 4 grid results in a 48 degree of freedom configuration space. In addition, the dynamics of the deformable object are difficult to model \citep{Essahbi2012}; even with high-fidelity modeling and simulation, planning for an individual task can take hours \citep{Bai2016}. Local controllers on the other hand are able to very efficiently generate motion, however, they are only able to successfully complete a task when the initial configuration is in the ``attraction basin'' of the goal \citep{Berenson2013,McConachie2018}. We propose combining the strengths of global planning with the strengths of local control while mitigating the weakness of each; we propose a framework for interleaving planning and control which uses global planning to generate gross motion of the deformable object, and a local controller to refine the configuration of the deformable object within the local neighborhood. By separating planning from control we are able to use different representations of the deformable object, each suited to efficient computation for their respective roles.

Two key ideas allow this framework to reliably perform tasks without a time-consuming modelling or data collection phase. First, we do not need to explicitly model and control every part of the deformable object, instead relying on the object's natural compliance in many situations. By doing so we drastically reduce the need for model fidelity, enabling the use of model approximations that do not need to be highly accurate. Second, our framework does not assume that the local controller or global planner are infallible. Instead, we assume that mistakes \textit{will} be made, and implement learning algorithms designed to avoid making the same mistake again. To this end I formulate the controller's task as a Multi-Armed Bandit problem, enabling the controller chose models based on the current circumstances. For the planner, I propose an online modeling method that improves the robot's model of the deformable object as mistakes are made, enabling the planner to avoid making the same mistake again.

This thesis will make six contributions towards answering these research questions:
\begin{itemize}
    \item {[Completed]} We introduce a more accurate geometric model of how  the direction of gripper motion and obstacles affect deformable objects.
    \item {[Completed]} We specify a novel stretching avoidance constraint to prevent the object from being overstretched by the robot as part of a local controller, allowing for the use of less accurate models without risking tearing the deformable object.
    \item {[Completed]} We formulate the task of the local controller as a Multi-Armed Bandit problem, with each arm representing a model of the deformable object.
    \item {[Completed]} We introduce a manipulation framework that interleaves planning and control, choosing each when most useful.
    \item {[Completed]} We present a global motion planner to generate gross motion of the deformable object, and provide a proof of probabilistic completeness for our planner, which is valid despite the fact that our system is underactuated and we do not have a steering function.
    \item {[Proposed]} We propose a method for improving the performance of the global planner as mistakes are made due to model approximations, enabling the planner to learn from experience.
\end{itemize}

% We present experiments in both a simulated environment and on a 16 DoF physical robot. Our results suggest that our planner can efficiently find paths, taking under a second on average to generate a feasible path in three out of four simulated scenarios. The physical experiment shows that our framework is able to effectively perform tasks in the real world, where reachability and dual-arm constraints make the planning more difficult.